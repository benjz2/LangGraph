{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import END, MessageGraph, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import List, Literal\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = MessageGraph()\n",
    "\n",
    "graph.add_node(\"oracle\", model)\n",
    "graph.add_edge(\"oracle\", END)\n",
    "\n",
    "graph.set_entry_point(\"oracle\")\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 1 + 1?', id='dbf71680-9847-4a69-ab1d-7cd27316f4af'),\n",
       " AIMessage(content='The answer is 2.', response_metadata={'token_usage': {'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens': 8}, 'model_name': 'accounts/fireworks/models/firefunction-v1', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-5e9afa14-d2ae-4a09-bffe-cd57f78216cc-0')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(HumanMessage(\"What is 1 + 1?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction with LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside for those already familiar with LangChain - `add_node` actually takes any function or [runnable](https://python.langchain.com/docs/expression_language/interface/) as input. In the above example, the model is used \"as-is\", but we could also have passed in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding a node to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Node `oracle` already present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_oracle\u001b[39m(messages: \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39minvoke(messages)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moracle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_oracle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langgraph/lib/python3.12/site-packages/langgraph/graph/state.py:69\u001b[0m, in \u001b[0;36mStateGraph.add_node\u001b[0;34m(self, key, action)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already being used as a state key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langgraph/lib/python3.12/site-packages/langgraph/graph/graph.py:116\u001b[0m, in \u001b[0;36mGraph.add_node\u001b[0;34m(self, key, action)\u001b[0m\n\u001b[1;32m    111\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding a node to a graph that has already been compiled. This will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be reflected in the compiled graph.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` already present.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m END \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m START:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is reserved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Node `oracle` already present."
     ]
    }
   ],
   "source": [
    "def call_oracle(messages: list):\n",
    "    return model.invoke(messages)\n",
    "\n",
    "graph.add_node(\"oracle\", call_oracle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just make sure you are mindful of the fact that the input to the [runnable](https://python.langchain.com/docs/expression_language/interface/) is the **entire current state**. So this will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding a node to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Node `oracle` already present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# State is a list of messages, but our chain expects a dict input:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# { \"name\": some_string, \"messages\": [] }\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Therefore, the graph will throw an exception when it executes here.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moracle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langgraph/lib/python3.12/site-packages/langgraph/graph/state.py:69\u001b[0m, in \u001b[0;36mStateGraph.add_node\u001b[0;34m(self, key, action)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already being used as a state key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langgraph/lib/python3.12/site-packages/langgraph/graph/graph.py:116\u001b[0m, in \u001b[0;36mGraph.add_node\u001b[0;34m(self, key, action)\u001b[0m\n\u001b[1;32m    111\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding a node to a graph that has already been compiled. This will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be reflected in the compiled graph.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` already present.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m END \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m START:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is reserved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Node `oracle` already present."
     ]
    }
   ],
   "source": [
    "# This will not work with MessageGraph!\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant named {name} who always speaks in pirate dialect\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "# State is a list of messages, but our chain expects a dict input:\n",
    "#\n",
    "# { \"name\": some_string, \"messages\": [] }\n",
    "#\n",
    "# Therefore, the graph will throw an exception when it executes here.\n",
    "graph.add_node(\"oracle\", chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(first_number: int, second_number: int):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return first_number * second_number\n",
    "\n",
    "model_with_tools = model.bind_tools([multiply])\n",
    "\n",
    "builder = MessageGraph()\n",
    "\n",
    "builder.add_node(\"oracle\", model_with_tools)\n",
    "\n",
    "tool_node = ToolNode([multiply])\n",
    "\n",
    "builder.add_node(\"multiply\", tool_node)\n",
    "\n",
    "builder.add_edge(\"multiply\", END)\n",
    "\n",
    "builder.set_entry_point(\"oracle\")\n",
    "\n",
    "def router(state: List[BaseMessage]) -> Literal[\"multiply\", \"__end__\"]:\n",
    "    tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    if len(tool_calls):\n",
    "        return \"multiply\"\n",
    "    else:\n",
    "        return \"__end__\"\n",
    "\n",
    "builder.add_conditional_edges(\"oracle\", router)\n",
    "\n",
    "runnable = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 123 * 456?', id='004ca911-bf64-4410-8e5a-c3552a0318f9'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_WQTTJx4O1P7jUd0vCIrfNlIH', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"first_number\": 123, \"second_number\": 456}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 211, 'total_tokens': 243, 'completion_tokens': 32}, 'model_name': 'accounts/fireworks/models/firefunction-v1', 'system_fingerprint': '', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-dea24fd6-518a-4827-9176-f5a26e3876a1-0', tool_calls=[{'name': 'multiply', 'args': {'first_number': 123, 'second_number': 456}, 'id': 'call_WQTTJx4O1P7jUd0vCIrfNlIH'}]),\n",
       " ToolMessage(content='56088', name='multiply', id='9945b21a-00bf-40f8-bf58-b0e777010a62', tool_call_id='call_WQTTJx4O1P7jUd0vCIrfNlIH')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(HumanMessage(\"What is 123 * 456?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is your name?', id='86db328c-d238-4735-a707-32ce61cb432f'),\n",
       " AIMessage(content=\"I am an AI and I don't have a personal name. I am here to assist you with any questions or tasks you have.\", response_metadata={'token_usage': {'prompt_tokens': 204, 'total_tokens': 234, 'completion_tokens': 30}, 'model_name': 'accounts/fireworks/models/firefunction-v1', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-ea97337f-44da-4a1a-9116-6a14704456c0-0')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(HumanMessage(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langgraph langchain_openai tavily-python\n",
    "# !pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [TavilySearchResults(max_results=1)]\n",
    "tool_node = ToolNode(tools)\n",
    "model = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", temperature=0)\n",
    "model = model.bind_tools(tools)\n",
    "\n",
    "def add_messages(left: list, right: list):\n",
    "    \"\"\"Add-don't-overwrite.\"\"\"\n",
    "    return left + right\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The `add_messages` function within the annotation defines\n",
    "    # *how* updates should be merged into the state.\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: AgentState) -> Literal[\"action\", \"__end__\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"action\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"action\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return \"__end__\"\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge('action', 'agent')\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_jYMnNrIa0oZ7bARyAxWNb9FJ', 'type': 'function', 'function': {'name': 'tavily_search_results_json', 'arguments': '{\"query\": \"weather in sf\"}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 203, 'total_tokens': 232, 'completion_tokens': 29}, 'model_name': 'accounts/fireworks/models/firefunction-v1', 'system_fingerprint': '', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-41528e29-30bf-47cf-b6d2-e371a30fb98f-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in sf'}, 'id': 'call_jYMnNrIa0oZ7bARyAxWNb9FJ'}]),\n",
       "  ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1715690473, \\'localtime\\': \\'2024-05-14 5:41\\'}, \\'current\\': {\\'last_updated_epoch\\': 1715689800, \\'last_updated\\': \\'2024-05-14 05:30\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Overcast\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/122.png\\', \\'code\\': 1009}, \\'wind_mph\\': 2.2, \\'wind_kph\\': 3.6, \\'wind_degree\\': 10, \\'wind_dir\\': \\'N\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 93, \\'cloud\\': 100, \\'feelslike_c\\': 10.8, \\'feelslike_f\\': 51.4, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 1.0, \\'gust_mph\\': 9.3, \\'gust_kph\\': 14.9}}\"}]', name='tavily_search_results_json', tool_call_id='call_jYMnNrIa0oZ7bARyAxWNb9FJ'),\n",
       "  AIMessage(content='The current weather in San Francisco, California, United States of America is overcast with a temperature of 11.7°C (53.1°F). The wind is coming from the North at 2.2 mph (3.6 kph) and the pressure is 1013.0 mb (29.92 in). The humidity is 93% and the visibility is 16 km (9 miles). The UV index is 1.0.', response_metadata={'token_usage': {'prompt_tokens': 676, 'total_tokens': 783, 'completion_tokens': 107}, 'model_name': 'accounts/fireworks/models/firefunction-v1', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-5c242ca3-f6a9-4ba9-93ac-056f88d02344-0')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_DFXYpsAVYZXwLne4cIQnccaD', 'type': 'function', 'function': {'name': 'tavily_search_results_json', 'arguments': '{\"query\": \"weather in sf\"}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 203, 'total_tokens': 232, 'completion_tokens': 29}, 'model_name': 'accounts/fireworks/models/firefunction-v1', 'system_fingerprint': '', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4135a8fd-99b2-4f88-be7e-90bae80acece-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in sf'}, 'id': 'call_DFXYpsAVYZXwLne4cIQnccaD'}])]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'action':\n",
      "---\n",
      "{'messages': [ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1715690494, \\'localtime\\': \\'2024-05-14 5:41\\'}, \\'current\\': {\\'last_updated_epoch\\': 1715689800, \\'last_updated\\': \\'2024-05-14 05:30\\', \\'temp_c\\': 11.7, \\'temp_f\\': 53.1, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Overcast\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/122.png\\', \\'code\\': 1009}, \\'wind_mph\\': 2.2, \\'wind_kph\\': 3.6, \\'wind_degree\\': 10, \\'wind_dir\\': \\'N\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 93, \\'cloud\\': 100, \\'feelslike_c\\': 10.8, \\'feelslike_f\\': 51.4, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 1.0, \\'gust_mph\\': 9.3, \\'gust_kph\\': 14.9}}\"}]', name='tavily_search_results_json', tool_call_id='call_DFXYpsAVYZXwLne4cIQnccaD')]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='The current weather in San Francisco, California, United States of America is overcast with a temperature of 11.7°C (53.1°F). The wind is coming from the North at 2.2 mph (3.6 kph) and the pressure is 1013.0 mb (29.92 in). The humidity is 93% and the visibility is 16 km (9 miles). The UV index is 1.0.', response_metadata={'token_usage': {'prompt_tokens': 676, 'total_tokens': 783, 'completion_tokens': 107}, 'model_name': 'accounts/fireworks/models/firefunction-v1', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-8cc25fbc-c9cb-4451-ad1f-ea4857b61f10-0')]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "for output in app.stream(inputs, stream_mode=\"updates\"):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content='I' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=\"'m sorry\" id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=',' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=' but I don' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=\"'t\" id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=' have the ability' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=' to provide weather' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=' information at the' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=' moment. Please' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=' try' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=' again' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content=' later.' id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n",
      "content='' response_metadata={'finish_reason': 'stop'} id='run-28fad127-971c-4dc2-86a3-1ebf2499e535'\n"
     ]
    }
   ],
   "source": [
    "model = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\")\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "async for output in app.astream_log(inputs, include_types=[\"llm\"]):\n",
    "    # astream_log() yields the requested logs (here LLMs) in JSONPatch format\n",
    "    for op in output.ops:\n",
    "        if op[\"path\"] == \"/streamed_output/-\":\n",
    "            # this is the output from .stream()\n",
    "            ...\n",
    "        elif op[\"path\"].startswith(\"/logs/\") and op[\"path\"].endswith(\n",
    "            \"/streamed_output/-\"\n",
    "        ):\n",
    "            # because we chose to only include LLMs, these are LLM tokens\n",
    "            print(op[\"value\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
